\chapter{Novo cluster: instalação e gerência}

\textbf{Motivação:} $\star\star\star\star\star$\\

\textbf{Tipo:} Projeto que se tornará atividade.\\

\textbf{Descrição:} Este projeto trata da instalação do novo cluster do Laboratório 
de Termohidráulica e Neutrônica do CDTN (LTHN). Este sistema compreende uma máquina mestra e 8 máquinas escravas, todas equipadas com placas gráficas Nvidia que podem ser utilizadas tanto para visualização quanto para cálculos em paralelo.

O sistema está parcialmente instalado com Linux CentOS 7.5 com um serviço de 
sistema de arquivos distribuído Gluster ativo. Atualmente o sistema também tem 
o sincronismo de timestamp entre nós e mestre. Isso é fundamental para evitar 
erros de sincronismo de arquivos (data mismatch).

O objetivo final deste projeto é ter o cluster funcionando com usuários com cotas 
de disco e com os seguintes sistemas instalados:

\begin{itemize}
	\item \textbf{Serpent2 Monte Carlo [INSTALADO]};
	\item \textbf{MCNP 6 [INSTALADO]};
	\item \textbf{OpenFOAM versão 6 [INSTALADO]};
	\item \textbf{Pacote ANSYS [INSTALADO]};
	\item SCALE
	\item \textbf{Matlab [INSTALADO (aguardando testes do Prana pelo André depois 
	da chegada da toolbox de processamento distribuído)]}.
\end{itemize}

No presente momento, o sistema NAS está instalado em modo padrão, com duas 
interfaces de rede 10Gbits conectadas as switch do cluster e uma interface 
1Gb conectada à rede do CDTN. As intefaces 10Gb estão unidas num \textit{bond}. 
O nome e endereço ip desse bond é \texttt{nas 13.13.13.60}. Há um compartilhamento 
NFS sendo usado pelo cluster que fica armazenado no NAS. Este compartilhamento 
está montado em todos os nós e é utilizado apenas como repositório dos pacotes 
slurm. Pode ser que seja interessante usar um compartilhamento NFS no NAS para 
outras aplicações no futuro, mas ainda não há definição sobre isso.

A autenticação é feita tanto com chaves públicas e privadas com SSH mas também 
está instalado o serviço RSH para autenticação sem criptografia. Isso funciona 
apenas internamente, entre máquinas do cluster e o master. O método 
\textit{hostbased authentication} falhou nas tentativas de configuração e, portanto, foi abandonado até um segundo momento, sendo utilizadas as duas 
formas acima de autenticação.\\

\textbf{Sub-atividades}

A partir da situação atual do cluster, as sequintes atividades estão previstas:
\begin{itemize}
%	\item[1] \textbf{Modificação da forma de autenticação dos hosts de chaves 
%	para autenticação no host}. Isso é fundamental para simplificar a criação 
%	de contas de usuários e diminuir o volume de dados de chaves armazenadas em 
%	um diretório \texttt{/home} compartilhado\footnote{"As configurações do \textit{host based authentication} falharam. Mais esforços serão empregados até um limite de tempo razoável. No entanto, caso o problema persista, será utilizada a solução atual com chaves públicas e primárias por usuário. Esta solução torna mais custosa a criação de contas de usuários, mas está funcional.};
	\item[1] Implementação de um script para adição (e remoção de usuários), levando-se em conta criar diretório em \texttt{/gv/data}, criar link simbólico, criar quota gluster, criação de usuário em todas as máquinas\footnote{Eventualmente pode ser implementado um sistema sem login para os nós mas com acesso via ssh aos nós. Isso seria útil pois reforçaria a necessidade do uso da submissão de \textit{jobs}.}.
%	\item[2] Finalizar os testes de armazenamento de dados de utilização do cluster (\texttt{sacct} e \texttt{sreport}).
%	\item[2] Configurar o \texttt{glusterdb} para armazenar informações \textit{log} de utilização de \textit{jobs}.
%	\item[2] Finalizar a instalação do Slurm\footnote{O Torque é o sistema padrão so CentOS. Entretanto, na literatura o Slurm é mais utilizado e será a provável escolha para o nosso cluster.} para lançamento de aplicações e controle de 
%	carga de trabalho do sistema;
	\item[2] Implementação de capacidade de visualização remota pelas placas gráficas a partir de outras máquinas Linux do LTHN.
	\item[3] Avaliação da viabilidade de um hub para exploração dos sistemas de segurança do cluster (power supplies e no-break).
	\item[4] Avaliação de implantação de desligamento automático em casa de falta de energia atráves de conexão lógica com o sistema de controle do no-break.
	\item[5] Configuração fina das dependências entre serviços de monitoramento 
	do cluster e do sistema de arquivos gluster, de modo a evitar que serviços 
	interdependentes sejam inicializados aleatoriamente quebrando eventual 
	dependência.
	\item[6] Preparar uma apresentação sobre o cluster e como utilizá-lo. Esta apresentação visa, inicialmente, a equipe (alunos e servidores) do LTHN.
\end{itemize}

Uma possível centralização de repositórios no NAS foi adiada, já que não se pretende, no curto prazo, promover atualizações de pacotes no Cluster uma vez 
funcional. Quando for interessante tecnicamente, isso será feito de modo a centralizar o repositório de aplicações no NAS, para que qualquer eventual instalação seja a partir dele, com apenas o playerone acessando a internet e atualizando esse repositório. Ele seria montado via NFS (já habilitado no NAS).

As sub-atividades listadas são atualizadas a cada revisão no documento, sendo que 
as sub-atividades finalizadas são removidas da lista e, de acordo com a demanda, 
são acrescidas novas tarefas. Por exemplo, a instalação do NAS ou a instalação do 
matlab, que não estavam no projeto inicial do cluster.

Foi instalado o Apache2 como servidor web no playerone para permitir o acompanhamento da carga no cluster via \textbf{Ganglia}. Este foi instalado 
e funciona perfeitamente. Foi feita uma página inicial com o logotipo do LTHN 
que leva até o Ganglia (\texttt{http://playerone.usuarios.cdtn.br}).

O \textit{Slurm} está instalado e funcional. Além disso, deverá ser preparada uma apresentação de como usar o cluster para \textbf{todos} os membros do LTHN. O sistema de armazenamento de dados de simulações oferecidos pelo \textit{Slurm} e controlado pelo serviço \texttt{slurmdbd} está funcional e os relatórios via \texttt{sacct} e \texttt{sreport} estão exibindo dados. \textbf{Está sob avaliação a instalação de um \textit{front-end} para visualização de dados armazenados no banco de dados de utilização do cluster.}

As sub-atividades de testes dos softwares instalados permanece, mas não mais como item. Isso se dá pois os testes dependem da disponibilidade dos usuários, da modelagem de casos de testes, etc. Até o presente momento, foram testados o MCNP6 (sem problemas detectados) e OpenFOAM (altíssima latência na decomposição da malha que pode inviabilizar simulações com muitos núcleos. Investigar pois pode estar ligado ao desempenho do \texttt{gluster}).

Foi ainda testado o SCALE, mas alguns dos seus componentes são disponíveis apenas 
em forma sequencial.\\

\textbf{Observações:}

O processo de instalação pode levar a demandas não previstas, de acordo com as avalições feitas para implantação de determinada \textit{feature} no sistema.

Uma vez instalado e testado, o projeto passa a ser uma atividade. Esta atividade 
consiste na manutenção e verificação do funcionamento do sistema.\\

\textbf{\textit{Outcomes}}

\begin{itemize}
	\item[1] Sistema de cálculos para as atividades do LTHN e, eventualmente, para usuários externos.
	\item[2] Paper INAC 2019 com descrição do processo final de instalação e exemplos de aplicações com tempo de execução.

\end{itemize}


\textbf{Dificuldades/Restrições}

O sistema está hoje fora da rede elétrica do gerador.\\

Em 13/12/2018 um dos equipamentos de ar-condionado da sala do cluster falhou. A equipe de manutenção do CDTN foi acionado no mesmo dia.\\

Em 25/01/2019 houve nova falha no sistema de ar-condicionado da sala do cluster e único aparelho funcional falhou, deixando o sistem sem refrigeração e fazendo com que a sala do cluster atingisse alta temperatura. Foi feito imediatamente um \textit{shutdown} emergencial - antes mesmo de verificar as temperaturas nos núcleos. O reparo dos equipamentos foi feito e, em 13/03/2019, 
o sistema de ar-condicionado funciona normalmente.\\

É necessário solicitar a instalação de uma tomada 110V para a ligação do hub 
(disponibilizado pelo Élcio) e dos equipamentos de rede que vão necessitar 
dessas conexãos.\\

\textbf{Em 13/03/2019: O nó \texttt{CORAGEM} segue apresentando erros que levam a um reboot que não termina, deixando a máquina em estado "hang". Vitor e Daniel estão responsáveis pelo contato e forma de acionamento da garantia da DELL. Está 
sendo investigada (via logs) a razão da pane de forma a fornecer detalhes 
do erro para a garantia DELL.}\\


%Última modificação: \today

%\today
